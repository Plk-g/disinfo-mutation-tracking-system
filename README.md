# Disinformation Topic Mutation Tracking System

**Course:** CSGY 6513 - Big Data  
**Team:** Palak Gupta, Eric Zhang, Samradnyee Shinde, Shreya Srinivasan Bharadwaj, Xiangping Liu

A real-time Big Data pipeline to detect, track, and visualize the evolution ("mutation") of online disinformation narratives across large-scale text sources. The system uses streaming data processing, NLP embeddings, and clustering to identify how misinformation claims transform as they spread through communities.

## ğŸ¯ Project Overview

This system tracks how disinformation narratives mutate over time by:
- Processing streaming text data from multiple sources
- Generating semantic embeddings using Sentence-BERT
- Clustering narratives and detecting topic drift
- Visualizing narrative evolution and mutation patterns
- Providing real-time analysis of news authenticity

## ğŸ—ï¸ System Architecture

```
Data Sources (PolitiFact, Reddit, FineWeb, GDELT, etc.)
         â†“
    Kafka Streaming
         â†“
  Spark Streaming (NLP + Clustering)
         â†“
    MongoDB Storage
         â†“
  Flask API + Web UI
         â†“
  Interactive Dashboard
```

### Technology Stack

- **Data Ingestion:** Apache Kafka (distributed streaming)
- **Processing:** Apache Spark Streaming (distributed computing)
- **Storage:** MongoDB Atlas (scalable NoSQL database)
- **NLP:** Sentence-BERT embeddings (384 dimensions)
- **ML:** K-means clustering, topic drift detection
- **Interface:** Flask web application with Chart.js visualizations

## ğŸ“‹ Features

### Core Functionality
- **Real-time Analysis:** Analyze news articles for authenticity (fake/real percentage)
- **Source Citations:** View similar narratives with source attribution
- **Mutation Tracking:** Track how disinformation narratives evolve over time
- **Interactive Dashboard:** Visualize mutation timelines and drift patterns
- **RESTful API:** Programmatic access to analysis results

### Data Sources
- PolitiFact (fact-checked misinformation)
- Reddit (social media posts)
- FineWeb (large web text corpus)
- GDELT (live news feed)
- Laxmimerit (fake news dataset)
- Synthetic data (for testing)

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- Java 8, 11, or 17 (for Spark)
- MongoDB Atlas account or local MongoDB
- Apache Kafka (for full pipeline)

### Installation

1. **Clone the repository:**
   ```bash
   git clone <repository-url>
   cd disinfo-mutation-tracking-system
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Set environment variables:**
   ```bash
   export MONGO_URI="mongodb+srv://<username>:<password>@cluster.mongodb.net/?retryWrites=true&w=majority"
   export MONGO_DB="disinfo_project"
   ```
   
   **Or use .env file:**
   ```bash
   cp .env.example .env
   # Edit .env with your credentials
   ```

4. **Create MongoDB indexes:**
   ```bash
   python scripts/create_indexes.py
   ```

### Running the System

#### Option 1: Quick Demo (No Kafka Required)

```bash
# Seed sample data
python scripts/seed_sample_data.py

# Start Flask application
python -m frontend.app

# Visit http://127.0.0.1:5001
```

#### Option 2: Full Pipeline

**Terminal 1 - Kafka Producer:**
```bash
python scripts/run_producer.py
```

**Terminal 2 - Spark Streaming:**
```bash
python main.py
```

**Terminal 3 - Flask Web UI:**
```bash
python -m frontend.app
```

## ğŸ“ Project Structure

```
disinfo-mutation-tracking-system/
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ db/                    # MongoDB database layer
â”‚       â”œâ”€â”€ mongo_client.py    # Database connection
â”‚       â””â”€â”€ queries.py         # Query functions
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app.py                 # Flask application
â”‚   â”œâ”€â”€ templates/             # HTML templates
â”‚   â”‚   â”œâ”€â”€ index.html         # Search interface
â”‚   â”‚   â”œâ”€â”€ results.html       # Analysis results
â”‚   â”‚   â””â”€â”€ mutations.html     # Mutation dashboard
â”‚   â””â”€â”€ static/
â”‚       â””â”€â”€ styles.css         # Styling
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ clustering/            # ML and analytics
â”‚       â”œâ”€â”€ clusterer.py       # K-means clustering
â”‚       â”œâ”€â”€ drift_model.py    # Topic drift detection
â”‚       â”œâ”€â”€ mutation_detector.py
â”‚       â”œâ”€â”€ embedding_generator.py  # Sentence-BERT
â”‚       â””â”€â”€ vector_utils.py    # Vector operations
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_producer.py        # Kafka producer
â”‚   â”œâ”€â”€ seed_sample_data.py    # Sample data generator
â”‚   â”œâ”€â”€ create_indexes.py     # MongoDB indexes
â”‚   â””â”€â”€ verify_system.py      # System verification
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ data_contract.md      # Data schema
â”‚   â”œâ”€â”€ storage_design.md     # Database design
â”‚   â”œâ”€â”€ ARCHITECTURE.md       # System architecture
â”‚   â””â”€â”€ SCALABILITY.md        # Scalability strategies
â”‚
â”œâ”€â”€ main.py                   # Spark streaming consumer
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                 # This file
```

## ğŸ”§ Configuration

### Kafka Configuration

Edit `scripts/run_producer.py` to select dataset:
```python
SELECTED_DATASET = "FINEWEB"  # Options: SYNTHETIC, POLITIFACT, REDDIT, FINEWEB, GDELT
TOPIC_NAME = "disinformation-stream"
KAFKA_BROKER = "localhost:9092"
```

### MongoDB Configuration

Set environment variables:
```bash
export MONGO_URI="your_mongodb_connection_string"
export MONGO_DB="disinfo_project"
```

## ğŸ“Š API Endpoints

### Web Interface
- `GET /` - Search interface
- `POST /search` - Analyze news article
- `GET /mutations` - Mutation dashboard

### REST API
- `GET /api/search?query=<text>&limit=20` - Search narratives
- `GET /api/top_claims?k=10` - Top claims by frequency
- `GET /api/claim/<claim_id>?limit=50` - Matches for a claim
- `GET /api/mutations/top?k=10` - Top mutations
- `GET /api/mutations/timeline?cluster_id=<id>` - Mutation timeline

## ğŸ§ª Testing

### System Verification
```bash
python scripts/verify_system.py
```

This tests:
- MongoDB connection
- NLP embeddings
- Clustering functionality
- Flask application
- Data visualization

### Quick Test
```bash
python scripts/quick_test.py
```

## ğŸ“ˆ Scalability

The system is designed to handle **millions to billions of records**:

- **Kafka:** Topic partitioning for parallel processing
- **Spark:** Distributed processing across cluster nodes
- **MongoDB:** Sharding for horizontal scaling
- **Processing Capacity:** 10,000+ messages/second (with proper scaling)

See `SCALABILITY.md` for detailed scalability strategies.

## ğŸ›ï¸ Architecture

For detailed architecture documentation, see:
- `ARCHITECTURE.md` - System architecture and data flow
- `SCALABILITY.md` - Scaling strategies and performance
- `docs/data_contract.md` - Data schema definitions

## ğŸ”’ Security

- Environment variables for credentials (never hardcoded)
- Input validation and sanitization
- Regex injection prevention
- Parameter validation on all API endpoints

## ğŸ“ Documentation

- `LOCAL_TESTING.md` - Local testing guide
- `SETUP.md` - Detailed setup instructions
- `TESTING.md` - Comprehensive testing guide
- `BUG_FIXES.md` - Bug fixes and improvements
- `PROJECT_REQUIREMENTS_ASSESSMENT.md` - Requirements analysis

## ğŸ¤ Contributing

This is a course project. For questions or issues, contact the team members.

## ğŸ“„ License

This project is part of CSGY 6513 - Big Data course work.

## ğŸ™ Acknowledgments

- Sentence-BERT for embeddings
- Apache Spark and Kafka communities
- MongoDB Atlas for database hosting
- Bootstrap and Chart.js for UI components

---

**Last Updated:** December 2025
