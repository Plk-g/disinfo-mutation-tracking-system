# Disinformation Mutation Tracking System

This project tracks how disinformation narratives **emerge, evolve, and mutate over time** using a streaming data pipeline and an interactive backend + frontend system.

It is designed to support:

* narrative similarity matching
* mutation / drift analysis
* exploratory search and visualization for downstream analysis


## ğŸ§  System Overview

**High-level flow:**

```
Kafka (raw posts)
   â†“
Spark Streaming (similarity + clustering)
   â†“
MongoDB (narrative_matches, mutation_events)
   â†“
Flask Backend + API
   â†“
Web UI & Visualization
```

The system is modular: each stage can be developed and tested independently.


## ğŸ—‚ï¸ Project Structure

```
disinfo-mutation-tracking-system/
â”‚
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ db/
â”‚       â”œâ”€â”€ mongo_client.py      # MongoDB connection (env-based)
â”‚       â””â”€â”€ queries.py           # Insert + query helpers
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app.py                   # Flask app + API routes
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â”œâ”€â”€ index.html
â”‚   â”‚   â”œâ”€â”€ results.html
â”‚   â”‚   â””â”€â”€ mutations.html
â”‚   â””â”€â”€ static/
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ data_contract.md         # Source-of-truth schema
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ create_indexes.py        # MongoDB indexes
â”‚   â”œâ”€â”€ smoke_test_db.py         # DB connectivity test
â”‚   â”œâ”€â”€ run_producer.py          # Kafka producer (sends sample narratives)
â”‚   â”œâ”€â”€ seed_sample_data.py      # Seed MongoDB with test data
â”‚   â””â”€â”€ run_complete_pipeline.py # Pipeline coordination script
â”œâ”€â”€ src/
â”‚   â””â”€â”€ clustering/
â”‚       â”œâ”€â”€ clusterer.py         # K-means clustering
â”‚       â”œâ”€â”€ drift_model.py       # Topic drift detection
â”‚       â”œâ”€â”€ mutation_detector.py # Mutation detection logic
â”‚       â”œâ”€â”€ embedding_generator.py # Sentence-BERT embeddings
â”‚       â””â”€â”€ vector_utils.py       # Vector operations
â”œâ”€â”€ main.py                      # Spark streaming consumer
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Set environment variables

> **Important:** credentials are never hardcoded.

```bash
export MONGO_URI="mongodb+srv://<username>:<password>@cluster0.jwaekxl.mongodb.net/?retryWrites=true&w=majority"
export MONGO_DB="disinfo_project"
```
### 3ï¸âƒ£ Create MongoDB indexes (one-time)

```bash
python3 scripts/create_indexes.py
```

Indexes include:

* text search on `text`
* `claim_id`
* `cluster_id`
* timestamps


### 4ï¸âƒ£ Run the application

```bash
python3 -m frontend.app
```

The app will start at:

```
http://127.0.0.1:5000
```

## ğŸŒ Demo Pages

### ğŸ” Search Interface

* **URL:** `/`
* Search narrative text and view:

  * similarity statistics
  * match rates
  * credibility score (heuristic)

### ğŸ”„ Mutation Dashboard

* **URL:** `/mutations`
* Displays:

  * top mutated narrative clusters
  * mutation score ranking
  * drift-over-time placeholder (API-driven)

If no mutation data exists yet, the page shows a clean **empty state**.

## ğŸ”Œ API Endpoints

These endpoints return JSON and are intended for visualization and analysis.

### Narrative search

```
GET /api/search?query=<text>&limit=20
```

### Top claims

```
GET /api/top_claims?k=10
```

### Matches for a claim

```
GET /api/claim/<claim_id>?limit=50
```

### Top mutations

```
GET /api/mutations/top?k=10
```

### Mutation timeline

```
GET /api/mutations/timeline?cluster_id=<id>
```

If no mutation data exists, **mock fallback data** is returned so demos never break.


## ğŸ§ª Smoke Test (Optional)

To verify MongoDB connectivity:

```bash
python3 -m scripts.smoke_test_db
```

## ğŸ“„ Data Contract

The authoritative schema definition lives in:

```
docs/data_contract.md
```

All upstream and downstream components are expected to conform to this contract.

## ğŸš€ Quick Start Guide

### Option 1: Test with Sample Data (No Kafka/Spark Required)

1. **Seed sample data:**
   ```bash
   python scripts/seed_sample_data.py
   ```

2. **Start Flask app:**
   ```bash
   python -m frontend.app
   ```

3. **Visit:** http://127.0.0.1:5000

### Option 2: Full Pipeline (Kafka + Spark + MongoDB)

**Prerequisites:**
- Kafka running on `localhost:9092`
- Java installed (for Spark)
- MongoDB accessible (MONGO_URI set)

**Terminal 1 - Kafka Producer:**
```bash
python scripts/run_producer.py
```

**Terminal 2 - Spark Streaming:**
```bash
python main.py
```

**Terminal 3 - Flask Web UI:**
```bash
python -m frontend.app
```

## ğŸš§ Current Status

* âœ… Backend DB layer complete
* âœ… Flask API complete
* âœ… Search + mutation UI ready
* âœ… Spark streaming pipeline with embeddings
* âœ… MongoDB integration
* âœ… Sample data seeder
* âœ… Cross-platform support (Windows/macOS/Linux)

---

## ğŸ‘©â€ğŸ’» Authors / Roles

* **Backend + Storage Lead:** Palak Gupta
* **Streaming / NLP / Visualization:** (team-specific)
